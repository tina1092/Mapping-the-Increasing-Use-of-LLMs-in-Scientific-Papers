{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimation import estimate_text_distribution, estimate_image_token_distribution\n",
    "from src.MLE import MLE\n",
    "\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new for image token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "# for each subject, estimate the distribution of human-written text and AI-generated text\n",
    "divisorList = [[1,0],[5,32],[5,64],[10,32],[32,32]]\n",
    "for name in [\"Pix2pix\"]:\n",
    "    for curdivisor in divisorList:\n",
    "        print(curdivisor)\n",
    "        estimate_image_token_distribution(f\"data/training_data/{name}/human_data.parquet\",\n",
    "                                          f\"data/training_data/{name}/ai_data.parquet\",\n",
    "                                          f\"distribution/{name}_value_{curdivisor[0]}_index_{curdivisor[1]}.parquet\",\n",
    "                                          valueDivisor = curdivisor[0],\n",
    "                                          indexDivisor = curdivisor[1]\n",
    "                                          )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing _value_1_index_0\n",
      "finish inference data\n",
      "Data saved to data/helper/inference_data_value_1_index_0.parquet\n",
      "finish data\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.726,     0.022,     0.726\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.726,     0.022,     0.701\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.726,     0.021,     0.676\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.726,     0.022,     0.651\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.725,     0.022,     0.625\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.727,     0.023,     0.602\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.726,     0.023,     0.576\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.726,     0.023,     0.551\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.727,     0.022,     0.527\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.726,     0.021,     0.501\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.727,     0.023,     0.477\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# for each subject, estimate the alpha value of mixed text and calculate the error\n",
    "divisorList = [[1,0],[5,32],[5,64],[10,32],[32,32]]\n",
    "\n",
    "for name in [\"Pix2pix\"]:\n",
    "    for curdivisor in divisorList:\n",
    "        sub_file_name = f\"_value_{curdivisor[0]}_index_{curdivisor[1]}\"\n",
    "        # load the framework\n",
    "        model=MLE(f\"distribution/{name}{sub_file_name}.parquet\")\n",
    "        print(f\"Processing {sub_file_name}\")\n",
    "        for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "            estimated,ci=model.image_inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\",\n",
    "                                         sub_file_name,\n",
    "                                         curdivisor[0],\n",
    "                                         curdivisor[1])\n",
    "            error=abs(estimated-alpha)\n",
    "            print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "            print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "        print(\"=====================================\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-101.78981657 -100.43770335 -102.18015721 ...  -99.79429943  -98.95448553\n",
      " -102.53336017]\n",
      "[-102.45183552 -100.68028784 -104.44290101 ...  -97.24816381  -99.99874495\n",
      " -103.80181687]\n",
      "[-100.05909024  -98.6141482   -99.20122139 ...  -97.73070386  -97.48056289\n",
      "  -99.08175025]\n",
      "[-103.4097598  -103.42605559 -103.42085011 ... -102.87438328 -101.62562455\n",
      " -103.06585302]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "divisorList = [[5,32],[5,64],[10,32],[32,32]]\n",
    "for curdivisor in divisorList:\n",
    "    # logp = np.load(f\"/home/ec2-user/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/data/helper/full_log_p_values_value_{curdivisor[0]}_index_{curdivisor[1]}.npy\")\n",
    "    logp = np.load(f\"/home/ec2-user/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/data/helper/full_log_p_value_{curdivisor[0]}_index_{curdivisor[1]}.npy\")\n",
    "    divide_value = logp[0]//-100\n",
    "    full_log_p_values = logp/divide_value\n",
    "    print(full_log_p_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37860\n",
      "validation:\n",
      "30000\n",
      "=====================================\n",
      "37537\n",
      "validation:\n",
      "30000\n",
      "=====================================\n",
      "27989\n",
      "validation:\n",
      "30000\n",
      "=====================================\n",
      "36112\n",
      "validation:\n",
      "30000\n",
      "=====================================\n",
      "21599\n",
      "validation:\n",
      "30000\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# for each subject, estimate the distribution of human-written text and AI-generated text\n",
    "for name in [\"CS\",\"EESS\",\"Math\",\"Phys\",\"Stat\"]:\n",
    "    human_source_path = f\"data/training_data/{name}/human_data.parquet\"\n",
    "    human_data=pd.read_parquet(human_source_path)\n",
    "    print(human_data.size)\n",
    "    print(\"validation:\")\n",
    "    for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "        validation = f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\"\n",
    "        validationdata=pd.read_parquet(validation)\n",
    "        \n",
    "        print(validationdata.size)\n",
    "        break\n",
    "    print(\"=====================================\")\n",
    "        #estimate_text_distribution(f\"data/training_data/{name}/human_data.parquet\",f\"data/training_data/{name}/ai_data.parquet\",f\"distribution/{name}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m=\u001b[39mMLE(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     datai,helpstr\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:166\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    164\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    168\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[1;32m    118\u001b[0m full_log_p_values \u001b[38;5;241m=\u001b[39m full_log_p_values\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m\n",
      "File \u001b[0;32m~/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:90\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "File \u001b[0;32m~/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 91\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_q_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "File \u001b[0;32m~/imageAI/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:91\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for each subject, estimate the alpha value of mixed text and calculate the error\n",
    "for name in [\"CS\",\"EESS\",\"Math\",\"Phys\",\"Stat\"]:\n",
    "    # load the framework\n",
    "    model=MLE(f\"distribution/{name}.parquet\")\n",
    "    for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "        datai,helpstr=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "        break\n",
    "    break\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        {with, changes, compatible, order, be, schedul...\n",
       "1        {embedding, how, our, tracking, reality, appli...\n",
       "2        {act, of, systems, the, trained, features, may...\n",
       "3        {key, of, transient, is, known, detections, re...\n",
       "4        {present, kernel, with, associated, of, is, re...\n",
       "                               ...                        \n",
       "29995    {these, nonlinear, of, because, networks, cruc...\n",
       "29996    {work, of, size, reducing, the, same, content,...\n",
       "29997    {interact, with, city, how, advice, the, which...\n",
       "29998    {with, change, of, be, is, quality, most, mann...\n",
       "29999    {sampled, through, achieves, novel, cascade, d...\n",
       "Name: inference_sentence, Length: 30000, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.000,     0.000,     0.000\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.000,     0.000,     0.025\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.000,     0.000,     0.050\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.000,     0.000,     0.075\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.000,     0.000,     0.100\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.000,     0.000,     0.125\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.000,     0.000,     0.150\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.000,     0.000,     0.175\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.000,     0.000,     0.200\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.006,     0.006,     0.219\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.043,     0.011,     0.207\n",
      "=====================================\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.000,     0.000,     0.000\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.000,     0.000,     0.025\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.000,     0.000,     0.050\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.000,     0.000,     0.075\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.000,     0.000,     0.100\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.000,     0.000,     0.125\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.000,     0.000,     0.150\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.000,     0.000,     0.175\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.000,     0.000,     0.200\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.010,     0.010,     0.215\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.045,     0.012,     0.205\n",
      "=====================================\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.000,     0.000,     0.000\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.000,     0.000,     0.025\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.000,     0.000,     0.050\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.001,     0.001,     0.074\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.006,     0.004,     0.094\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.029,     0.006,     0.096\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.053,     0.007,     0.097\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.083,     0.008,     0.092\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.117,     0.009,     0.083\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.154,     0.010,     0.071\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.185,     0.010,     0.065\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "# for each subject, estimate the alpha value of mixed text and calculate the error\n",
    "for name in [\"CS\",\"EESS\",\"Math\",\"Phys\",\"Stat\"]:\n",
    "    # load the framework\n",
    "    model=MLE(f\"distribution/{name}.parquet\")\n",
    "    for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "        estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "        error=abs(estimated-alpha)\n",
    "        print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "        print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
